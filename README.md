# Drug-discovery-using-Biopython-ML-and-Data-Analysis

ğŸ’Š ML & Data Analysis in Drug Discovery (ChEMBL + QSAR)

**QSAR (Quantitative Structureâ€“Activity Relationship)** pipeline end-to-end:
âœ… fetch bioactivity data â†’ âœ… clean & transform â†’ âœ… build molecular features â†’ âœ… train ML models â†’ âœ… compare results

---

## ğŸ§­ What this project does

### 1) ğŸ” Collect bioactivity data (ChEMBL)
- Search a target in ChEMBL and pull **IC50** activity records for compounds.

### 2) ğŸ§¹ Clean + prepare the dataset
- Keep key columns (e.g., ChEMBL ID, SMILES, IC50)
- Convert IC50 into **pIC50** (more stable for modeling)
- Label compounds into **bioactivity classes** (active / inactive / intermediate)

### 3) ğŸ§ª Engineer molecular features
- **Lipinski descriptors (RDKit)**: MW, LogP, HBD, HBA
- **Fingerprints (PaDEL / PubChem)**: binary feature vectors for ML

### 4) ğŸ“Š Explore chemical space (EDA)
- Basic distribution plots and class comparisons
- **Mannâ€“Whitney U tests** to check whether descriptor distributions differ between classes

### 5) ğŸ¤– Train & benchmark ML models
- Build a baseline QSAR regression model
- Compare many regressors automatically using **LazyPredict**

---

## âœ¨ Key insights from the notebook

Here are the main takeaways observed in the saved notebook run:

### ğŸ“Œ Insight 1: Actives vs inactives arenâ€™t â€œthe sameâ€ statistically
Using **Mannâ€“Whitney U tests**:
- âœ… **pIC50** shows a **different distribution** between actives and inactives (reject H0)
- âœ… **Molecular Weight (MW)** differs significantly (reject H0)
- âœ… **H-bond donors (HBD)** differ significantly (reject H0)
- âœ… **H-bond acceptors (HBA)** differ significantly (reject H0)
- âš–ï¸ **LogP** appears **similar** between actives and inactives here (fail to reject H0)

### ğŸ“Œ Insight 2: Feature filtering matters a lot
- The fingerprint feature space starts large (**881 bits**)
- After variance filtering, it shrinks to **137 informative features** ğŸ§ 

### ğŸ“Œ Insight 3: Baseline models achieve moderate predictability
- Best models reach about **RÂ² â‰ˆ 0.54** on the test set
- Error remains meaningful (RMSE ~ **1.06**), so improvements like better splitting/tuning can help ğŸš€

---

## ğŸ Results snapshot (from the saved notebook run)

**Modeling dataset (fingerprints):**
- **4695 compounds**
- **881 fingerprint bits** â†’ after variance filtering: **137 features**
- Split: **80/20** (3756 / 939)

**Baseline:**
- ğŸŒ² Random Forest test **RÂ² â‰ˆ 0.54**

**Top test-set models (LazyPredict):**
- ğŸŸ© HistGradientBoostingRegressor: **RÂ² 0.54**, RMSE 1.06
- ğŸŸ¦ LGBMRegressor (LightGBM): **RÂ² 0.54**, RMSE 1.06
- ğŸŒ² RandomForestRegressor: **RÂ² 0.52**, RMSE 1.08

> Interpretation: the models capture real structureâ€“activity signal, but thereâ€™s room for better generalization and tuning.

---

## ğŸ§° Tech stack

### ğŸ Core tools & libraries
- Python, Jupyter Notebook
- pandas, numpy
- scikit-learn
- matplotlib / seaborn
- `chembl_webresource_client` (ChEMBL data access)
- RDKit (Lipinski descriptors)
- PaDEL-Descriptor (fingerprints; requires Java)
- LazyPredict (quick multi-model benchmarking)

### ğŸ¤– ML models used (notebook)
**Baseline model**
- RandomForestRegressor ğŸŒ²

**Benchmarked via LazyPredict**
- HistGradientBoostingRegressor
- GradientBoostingRegressor
- LGBMRegressor (LightGBM)
- XGBRegressor (XGBoost)
- SVR / NuSVR
- KNeighborsRegressor
- MLPRegressor
- BaggingRegressor
- ExtraTreesRegressor
- (plus additional linear models like Ridge / Lasso variants)

---

## âš™ï¸ How to run

1) Clone the repo and open the notebook:
```bash
git clone <your-repo-url>
cd <your-repo-folder>
jupyter notebook
